{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing mikolaj continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdotenv\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dotenv, find_dotenv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='90 3. Linear Regression\\n0 50 100 150200 600 1000 1400IncomeBalance0 50 100 150200 600 1000 1400IncomeBalancestudentnon−student\\nFIGURE 3.7.For theCreditdata, the least squares lines are shown for pre-diction ofbalancefromincomefor students and non-students.Left:The model(3.34) was ﬁt. There is no interaction betweenincomeandstudent.Right:Themodel (3.35) was ﬁt. There is an interaction term betweenincomeandstudent.takes the formbalancei≈β0+β1×incomei+/braceleft⎪iggβ2ifith person is a student0 ifith person is not a student=β1×incomei+/braceleft⎪iggβ0+β2ifith person is a studentβ0ifith person is not a student.(3.34)Notice that this amounts to ﬁtting two parallel lines to the data, one forstudents and one for non-students. The lines for students and non-studentshave diﬀerent intercepts,β0+β2versusβ0, but the same slope,β1. Thisis illustrated in the left-hand panel of Figure 3.7. The fact that the linesare parallel means that the average eﬀect onbalanceof a one-unit increaseinincomedoes not depend on whether or not the individual is a student.This represents a potentially serious limitation of the model, since in fact achange inincomemay have a very diﬀerent eﬀect on the credit card balanceof a student versus a non-student.This limitation can be addressed by adding an interaction variable, cre-ated by multiplyingincomewith the dummy variable forstudent. Ourmodel now becomesbalancei≈β0+β1×incomei+/braceleft⎪iggβ2+β3×incomeiif student0 if not student=/braceleft⎪igg(β0+β2)+(β1+β3)×incomeiif studentβ0+β1×incomeiif not student.(3.35)', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 99})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'data/pdfs/ISLRv2_website.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file)\n",
    "document = loader.load()\n",
    "document[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='FIGURE 2.9.Left:Data simulated fromf, shown in black. Three estimates offare shown: the linear regression line (orange curve), and two smoothing splineﬁts (blue and green curves).Right:Training MSE (grey curve), test MSE (redcurve), and minimum possible test MSE over all methods (dashed line). Squaresrepresent the training and test MSEs for the three ﬁts shown in the left-handpanel.since the training MSE and the test MSE appear to be closely related.Unfortunately, there is a fundamental problem with this strategy: thereis no guarantee that the method with the lowest training MSE will alsohave the lowest test MSE. Roughly speaking, the problem is that manystatistical methods speciﬁcally estimate coeﬃcients so as to minimize thetraining set MSE. For these methods, the training set MSE can be quitesmall, but the test MSE is often much larger.Figure 2.9 illustrates this phenomenon on a simple example. In the left-hand panel of Figure 2.9, we have generated observations from (2.1) withthe truefgiven by the black curve. The orange, blue and green curves illus-trate three possible estimates forfobtained using methods with increasinglevels of ﬂexibility. The orange line is the linear regression ﬁt, which is rela-tively inﬂexible. The blue and green curves were produced usingsmoothingsplines, discussed in Chapter 7, with diﬀerent levels of smoothness. It issmoothingsplineclear that as the level of ﬂexibility increases, the curves ﬁt the observeddata more closely. The green curve is', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 41})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "splits = text_splitter.split_documents(document)\n",
    "splits[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the pages into embeddings and store in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 14:42:18.233247: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 14:42:20.724440: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-03 14:42:25.820079: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 14:42:25.822743: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-03 14:42:25.822772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    splits, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query a store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose a query, find pages similar to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Explain bias-variance tradeoff for a linear regression model.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='bias-variancetrade-oﬀ in mind. In this book we explore methods that are extremelyﬂexible and hence can essentially eliminate bias. However, this does notguarantee that they will outperform a much simpler method such as linearregression. To take an extreme example, suppose that the truefis linear.In this situation linear regression will have no bias, making it very hardfor a more ﬂexible method to compete. In contrast, if the truefis highlynon-linear and we have an ample number of training observations, thenwe may do better using a highly ﬂexible approach, as in Figure 2.11. InChapter 5 we discuss cross-validation, which is a way to estimate the testMSE using the training data.', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 46}),\n",
       " Document(page_content='2.2 Assessing Model Accuracy 35one of these data points may cause the estimateˆfto change considerably.In contrast, the orange least squares line is relatively inﬂexible and has lowvariance, because moving any single observation will likely cause only asmall shift in the position of the line.On the other hand,biasrefers to the error that is introduced by approxi-mating a real-life problem, which may be extremely complicated, by a muchsimpler model. For example, linear regression assumes that there is a linearrelationship betweenYandX1,X2,...,Xp. It is unlikely that any real-lifeproblem truly has such a simple linear relationship, and so performing lin-ear regression will undoubtedly result in some bias in the estimate off. InFigure 2.11, the truefis substantially non-linear, so no matter how manytraining observations we are given, it will not be possible to produce anaccurate estimate using linear regression. In other words, linear regressionresults in high bias in this example. However, in Figure 2.10 the truefisvery close to linear, and so given enough data, it should be possible forlinear regression to produce an accurate estimate. Generally, more ﬂexiblemethods result in less bias.As a general rule, as we use more ﬂexible methods, the variance willincrease and the bias will decrease. The relative rate of change of thesetwo quantities determines whether the test MSE increases or decreases. Aswe increase the ﬂexibility of a class of methods, the bias tends to', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 45}),\n",
       " Document(page_content='FIGURE 2.11.Details are as in Figure 2.9, using a diﬀerentfthat is far fromlinear. In this setting, linear regression provides a very poor ﬁt to the data.termsϵ. That is,E/parenleft⎪igy0−ˆf(x0)/parenright⎪ig2= Var(ˆf(x0)) + [Bias(ˆf(x0))]2+ Var(ϵ).(2.7)Here the notationE/parenleft⎪igy0−ˆf(x0)/parenright⎪ig2deﬁnes theexpected test MSEatx0,expectedtest MSEand refers to the average test MSE that we would obtain if we repeatedlyestimatedfusing a large number of training sets, and tested each atx0. Theoverall expected test MSE can be computed by averagingE/parenleft⎪igy0−ˆf(x0)/parenright⎪ig2over all possible values ofx0in the test set.Equation 2.7 tells us that in order to minimize the expected test error,we need to select a statistical learning method that simultaneously achieveslow varianceandlow bias. Note that variance is inherently a nonnegativequantity, and squared bias is also nonnegative. Hence, we see that theexpected test MSE can never lie below Var(ϵ), the irreducible error from(2.3).What do we mean by thevarianceandbiasof a statistical learningmethod?Variancerefers to the amount by whichˆfwould change if weestimated it using a diﬀerent training data set. Since the training dataare used to ﬁt the statistical learning method, diﬀerent training data setswill result in a diﬀerentˆf. But ideally the estimate forfshould not varytoo much between training sets. However, if a method has high variancethen small changes in the training data can result in large changes inˆf.', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 44}),\n",
       " Document(page_content='FIGURE 2.12.Squared bias (blue curve), variance (orange curve), Var(ϵ)(dashed line), and test MSE (red curve) for the three data sets in Figures 2.9–2.11.The vertical dotted line indicates the ﬂexibility level corresponding to the smallesttest MSE.as ﬂexibility increases. Consequently, the test MSE declines substantiallybefore experiencing a small increase as model ﬂexibility increases.The relationship between bias, variance, and test set MSE given in Equa-tion 2.7 and displayed in Figure 2.12 is referred to as thebias-variancetrade-oﬀ. Good test set performance of a statistical learning method re-bias-variancetrade-oﬀquires low variance as well as low squared bias. This is referred to as atrade-oﬀ because it is easy to obtain a method with extremely low bias buthigh variance (for instance, by drawing a curve that passes through everysingle training observation) or a method with very low variance but highbias (by ﬁtting a horizontal line to the data). The challenge lies in ﬁndinga method for which both the variance and the squared bias are low. Thistrade-oﬀ is one of the most important recurring themes in this book.In a real-life situation in whichfis unobserved, it is generally not pos-sible to explicitly compute the test MSE, bias, or variance for a statisticallearning method. Nevertheless, one should always keep the bias-variancetrade-oﬀ in mind. In this book we explore methods that are extremelyﬂexible and hence can essentially eliminate bias. However, this does', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 46})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_pages = db.similarity_search(query, limit=10)\n",
    "similar_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the stored pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "\n",
    "qpages = \"\".join([similar_pages[i].page_content for i in range(len(similar_pages))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qpages} Question: {query}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The bias-variance tradeoff for a linear regression model refers to the relationship between the model's bias and variance. \n",
       "\n",
       "Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. In the case of linear regression, it assumes a linear relationship between the dependent variable and the independent variables. If the true relationship is not linear, using linear regression will result in bias.\n",
       "\n",
       "Variance, on the other hand, refers to the amount by which the estimated regression line would change if we estimated it using a different training dataset. If a model has high variance, small changes in the training data can result in large changes in the estimated regression line.\n",
       "\n",
       "The tradeoff occurs because as we increase the flexibility of the model, such as by using more complex regression models, the bias tends to decrease but the variance tends to increase. This means that more flexible models can better capture the true relationship between the variables, reducing bias, but they are also more sensitive to the specific training data, increasing variance.\n",
       "\n",
       "In the case of linear regression, it is a relatively simple and less flexible model. It assumes a linear relationship between the variables and has low variance. However, if the true relationship is highly non-linear, linear regression will have high bias and may not accurately capture the true relationship.\n",
       "\n",
       "Therefore, the bias-variance tradeoff for a linear regression model is that it has low variance but potentially high bias, especially if the true relationship is non-linear."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
