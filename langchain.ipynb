{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/pdfs/ISLRv2_website.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='90 3. Linear Regression\\n0 50 100 150200 600 1000 1400IncomeBalance0 50 100 150200 600 1000 1400IncomeBalancestudentnon−student\\nFIGURE 3.7.For theCreditdata, the least squares lines are shown for pre-diction ofbalancefromincomefor students and non-students.Left:The model(3.34) was ﬁt. There is no interaction betweenincomeandstudent.Right:Themodel (3.35) was ﬁt. There is an interaction term betweenincomeandstudent.takes the formbalancei≈β0+β1×incomei+/braceleft⎪iggβ2ifith person is a student0 ifith person is not a student=β1×incomei+/braceleft⎪iggβ0+β2ifith person is a studentβ0ifith person is not a student.(3.34)Notice that this amounts to ﬁtting two parallel lines to the data, one forstudents and one for non-students. The lines for students and non-studentshave diﬀerent intercepts,β0+β2versusβ0, but the same slope,β1. Thisis illustrated in the left-hand panel of Figure 3.7. The fact that the linesare parallel means that the average eﬀect onbalanceof a one-unit increaseinincomedoes not depend on whether or not the individual is a student.This represents a potentially serious limitation of the model, since in fact achange inincomemay have a very diﬀerent eﬀect on the credit card balanceof a student versus a non-student.This limitation can be addressed by adding an interaction variable, cre-ated by multiplyingincomewith the dummy variable forstudent. Ourmodel now becomesbalancei≈β0+β1×incomei+/braceleft⎪iggβ2+β3×incomeiif student0 if not student=/braceleft⎪igg(β0+β2)+(β1+β3)×incomeiif studentβ0+β1×incomeiif not student.(3.35)', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 99})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the pages into embeddings and store in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mesonyktio/.cache/pypoetry/virtualenvs/chat-with-my-data-5mIFhxiN-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-27 23:01:13.262005: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-27 23:01:15.608939: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-27 23:01:15.609024: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-27 23:01:15.609031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    pages, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query a store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose a query, find pages similar to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Explain bias-variance tradeoff for a linear regression model.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2.2 Assessing Model Accuracy 35one of these data points may cause the estimateˆfto change considerably.In contrast, the orange least squares line is relatively inﬂexible and has lowvariance, because moving any single observation will likely cause only asmall shift in the position of the line.On the other hand,biasrefers to the error that is introduced by approxi-mating a real-life problem, which may be extremely complicated, by a muchsimpler model. For example, linear regression assumes that there is a linearrelationship betweenYandX1,X2,...,Xp. It is unlikely that any real-lifeproblem truly has such a simple linear relationship, and so performing lin-ear regression will undoubtedly result in some bias in the estimate off. InFigure 2.11, the truefis substantially non-linear, so no matter how manytraining observations we are given, it will not be possible to produce anaccurate estimate using linear regression. In other words, linear regressionresults in high bias in this example. However, in Figure 2.10 the truefisvery close to linear, and so given enough data, it should be possible forlinear regression to produce an accurate estimate. Generally, more ﬂexiblemethods result in less bias.As a general rule, as we use more ﬂexible methods, the variance willincrease and the bias will decrease. The relative rate of change of thesetwo quantities determines whether the test MSE increases or decreases. Aswe increase the ﬂexibility of a class of methods, the bias tends to initiallydecrease faster than the variance increases. Consequently, the expectedtest MSE declines. However, at some point increasing ﬂexibility has littleimpact on the bias but starts to signiﬁcantly increase the variance. Whenthis happens the test MSE increases. Note that we observed this patternof decreasing test MSE followed by increasing test MSE in the right-handpanels of Figures 2.9–2.11.The three plots in Figure 2.12 illustrate Equation 2.7 for the examples inFigures 2.9–2.11. In each case the blue solid curve represents the squaredbias, for diﬀerent levels of ﬂexibility, while the orange curve corresponds tothe variance. The horizontal dashed line represents Var(ϵ), the irreducibleerror. Finally, the red curve, corresponding to the test set MSE, is the sumof these three quantities. In all three cases, the variance increases and thebias decreases as the method’s ﬂexibility increases. However, the ﬂexibilitylevel corresponding to the optimal test MSE diﬀers considerably among thethree data sets, because the squared bias and variance change at diﬀerentrates in each of the data sets. In the left-hand panel of Figure 2.12, thebias initially decreases rapidly, resulting in an initial sharp decrease in theexpected test MSE. On the other hand, in the center panel of Figure 2.12the truefis close to linear, so there is only a small decrease in bias as ﬂex-ibility increases, and the test MSE only declines slightly before increasingrapidly as the variance increases. Finally, in the right-hand panel of Fig-ure 2.12, as ﬂexibility increases, there is a dramatic decline in bias becausethe truefis very non-linear. There is also very little increase in variance', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 45}),\n",
       " Document(page_content='240 6. Linear Model Selection and Regularization\\n1e−01 1e+01 1e+030 10 20 30 40 50 60Mean Squared Error0.0 0.2 0.4 0.6 0.8 1.00 10 20 30 40 50 60Mean Squared Errorλ∥ˆβRλ∥2/∥ˆβ∥2FIGURE 6.5.Squared bias (black), variance (green), and test mean squarederror (purple) for the ridge regression predictions on a simulated data set, as afunction ofλand∥ˆβRλ∥2/∥ˆβ∥2. The horizontal dashed lines indicate the minimumpossible MSE. The purple crosses indicate the ridge regression models for whichthe MSE is smallest.function ofλ. At the least squares coeﬃcient estimates, which correspondto ridge regression withλ= 0, the variance is high but there is no bias. Butasλincreases, the shrinkage of the ridge coeﬃcient estimates leads to asubstantial reduction in the variance of the predictions, at the expense of aslight increase in bias. Recall that the test mean squared error (MSE), plot-ted in purple, is closely related to the variance plus the squared bias. Forvalues ofλup to about 10, the variance decreases rapidly, with very littleincrease in bias, plotted in black. Consequently, the MSE drops consider-ably asλincreases from 0 to 10. Beyond this point, the decrease in variancedue to increasingλslows, and the shrinkage on the coeﬃcients causes themto be signiﬁcantly underestimated, resulting in a large increase in the bias.The minimum MSE is achieved at approximatelyλ= 30. Interestingly,because of its high variance, the MSE associated with the least squaresﬁt, whenλ= 0, is almost as high as that of the null model for which allcoeﬃcient estimates are zero, whenλ=∞. However, for an intermediatevalue ofλ, the MSE is considerably lower.The right-hand panel of Figure 6.5 displays the same curves as the left-hand panel, this time plotted against theℓ2norm of the ridge regressioncoeﬃcient estimates divided by theℓ2norm of the least squares estimates.Now as we move from left to right, the ﬁts become more ﬂexible, and sothe bias decreases and the variance increases.In general, in situations where the relationship between the responseand the predictors is close to linear, the least squares estimates will havelow bias but may have high variance. This means that a small change inthe training data can cause a large change in the least squares coeﬃcientestimates. In particular, when the number of variablespis almost as largeas the number of observationsn, as in the example in Figure 6.5, theleast squares estimates will be extremely variable. And ifp>n, then the', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 247}),\n",
       " Document(page_content='36 2. Statistical Learning\\n2 5 10 200.0 0.5 1.0 1.5 2.0 2.5Flexibility2 5 10 200.0 0.5 1.0 1.5 2.0 2.5Flexibility2 5 10 200 5 10 15 20FlexibilityMSEBiasVar\\nFIGURE 2.12.Squared bias (blue curve), variance (orange curve), Var(ϵ)(dashed line), and test MSE (red curve) for the three data sets in Figures 2.9–2.11.The vertical dotted line indicates the ﬂexibility level corresponding to the smallesttest MSE.as ﬂexibility increases. Consequently, the test MSE declines substantiallybefore experiencing a small increase as model ﬂexibility increases.The relationship between bias, variance, and test set MSE given in Equa-tion 2.7 and displayed in Figure 2.12 is referred to as thebias-variancetrade-oﬀ. Good test set performance of a statistical learning method re-bias-variancetrade-oﬀquires low variance as well as low squared bias. This is referred to as atrade-oﬀ because it is easy to obtain a method with extremely low bias buthigh variance (for instance, by drawing a curve that passes through everysingle training observation) or a method with very low variance but highbias (by ﬁtting a horizontal line to the data). The challenge lies in ﬁndinga method for which both the variance and the squared bias are low. Thistrade-oﬀ is one of the most important recurring themes in this book.In a real-life situation in whichfis unobserved, it is generally not pos-sible to explicitly compute the test MSE, bias, or variance for a statisticallearning method. Nevertheless, one should always keep the bias-variancetrade-oﬀ in mind. In this book we explore methods that are extremelyﬂexible and hence can essentially eliminate bias. However, this does notguarantee that they will outperform a much simpler method such as linearregression. To take an extreme example, suppose that the truefis linear.In this situation linear regression will have no bias, making it very hardfor a more ﬂexible method to compete. In contrast, if the truefis highlynon-linear and we have an ample number of training observations, thenwe may do better using a highly ﬂexible approach, as in Figure 2.11. InChapter 5 we discuss cross-validation, which is a way to estimate the testMSE using the training data.', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 46}),\n",
       " Document(page_content='34 2. Statistical Learning\\n0 20 40 60 80 100−10 0 10 20XY\\n2 5 10 200 5 10 15 20FlexibilityMean Squared Error\\nFIGURE 2.11.Details are as in Figure 2.9, using a diﬀerentfthat is far fromlinear. In this setting, linear regression provides a very poor ﬁt to the data.termsϵ. That is,E/parenleft⎪igy0−ˆf(x0)/parenright⎪ig2= Var(ˆf(x0)) + [Bias(ˆf(x0))]2+ Var(ϵ).(2.7)Here the notationE/parenleft⎪igy0−ˆf(x0)/parenright⎪ig2deﬁnes theexpected test MSEatx0,expectedtest MSEand refers to the average test MSE that we would obtain if we repeatedlyestimatedfusing a large number of training sets, and tested each atx0. Theoverall expected test MSE can be computed by averagingE/parenleft⎪igy0−ˆf(x0)/parenright⎪ig2over all possible values ofx0in the test set.Equation 2.7 tells us that in order to minimize the expected test error,we need to select a statistical learning method that simultaneously achieveslow varianceandlow bias. Note that variance is inherently a nonnegativequantity, and squared bias is also nonnegative. Hence, we see that theexpected test MSE can never lie below Var(ϵ), the irreducible error from(2.3).What do we mean by thevarianceandbiasof a statistical learningmethod?Variancerefers to the amount by whichˆfwould change if weestimated it using a diﬀerent training data set. Since the training dataare used to ﬁt the statistical learning method, diﬀerent training data setswill result in a diﬀerentˆf. But ideally the estimate forfshould not varytoo much between training sets. However, if a method has high variancethen small changes in the training data can result in large changes inˆf. Ingeneral, more ﬂexible statistical methods have higher variance. Consider thegreen and orange curves in Figure 2.9. The ﬂexible green curve is followingthe observations very closely. It has high variance because changing any', metadata={'source': 'data/pdfs/ISLRv2_website.pdf', 'page': 44})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_pages = db.similarity_search(query, limit=10)\n",
    "similar_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the stored pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "\n",
    "qpages = \"\".join([similar_pages[i].page_content for i in range(len(similar_pages))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qpages} Question: Explain bias-variance tradeoff for a linear regression model.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The bias-variance tradeoff refers to the relationship between the bias and variance of a statistical learning model. In the context of a linear regression model, bias refers to the error introduced by approximating a real-life problem with a simpler linear model. It represents the difference between the true relationship between the predictors and the response and the relationship estimated by the linear regression model. On the other hand, variance refers to the variability of the model's predictions when trained on different datasets. It represents the sensitivity of the model to changes in the training data.\n",
       "\n",
       "In general, more flexible models, such as those with more complex relationships or more parameters, tend to have lower bias but higher variance. This means that they can capture more intricate patterns in the data but are also more likely to overfit the training data and perform poorly on new, unseen data. On the other hand, less flexible models, such as simple linear regression, have higher bias but lower variance. They may not capture all the nuances in the data but are more robust and generalize better to new data.\n",
       "\n",
       "The tradeoff occurs because as we increase the flexibility of a model, the bias tends to decrease faster than the variance increases, resulting in a decrease in the expected test mean squared error (MSE). However, at some point, increasing flexibility has little impact on the bias but significantly increases the variance, causing the test MSE to increase. The goal is to find the optimal level of flexibility that minimizes the test MSE, striking a balance between bias and variance.\n",
       "\n",
       "In the case of linear regression, if the true relationship between the predictors and the response is close to linear, the model will have low bias but may have high variance. This means that small changes in the training data can cause large changes in the estimated coefficients. On the other hand, if the true relationship is highly non-linear, a more flexible model may be able to capture the complexity and reduce bias, resulting in a lower test MSE.\n",
       "\n",
       "Overall, the bias-variance tradeoff highlights the importance of finding the right level of model complexity that balances bias and variance to achieve the best predictive performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
